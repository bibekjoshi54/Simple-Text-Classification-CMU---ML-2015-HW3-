Hw3 Solution!
===================
This is the python solution for <a> http://www.cs.cmu.edu/~ninamf/courses/601sp15/hw/homework3.pdf</a>

----------

Problem Statement
-------------
In this question, you will implement a Naive Bayes classifier for a text classification problem. You will be given a collection of text articles, each coming from either the serious European magazine The Economist or from the not-so-serious American magazine The Onion. The goal is to learn a classifier that can distinguish between articles from each magazine. We have pre-processed the articles so that they are easier to use in your experiments. We extracted the set of all words that occur in any of the articles. This set is called the vocabulary and we let V be the number of words in the vocabulary. For each article, we produced a feature vector X =< X<sub>1</sub>, . . . , X<sub>V</sub>>, where Xi is equal to 1 if the i<sup>th</sup> word appears in the article and 0 otherwise. Each article is also accompanied by a class label of either 1 for The Economist or 2 for The Onion.

<b><strong> The assignment is done in<i> python </i> instead of <i>octave </i> .</b></strong>

________
Solution
-------------
I have changed the lavel from (1,2) to (0,1)
### Implementing genCsv
It is responsible for generating csv from given MAT File
### Implementing logprod
The log prod is responsible for returing the log of any product in the array
<code>
<pre>
def logProd(x):
   return x.sum() 
  </pre>
</code>

### YPrior(yTrain)

Used To calculate the prior for y 
<code>
<pre>
def prior(y):
    return y[y['0'] == 0].shape[0]/y.shape[0]</pre>
   </code>

### Implementing  XGivenY(XTrain, yTrain)
The output D is a 2 × V matrix, where for any word index w ∈ {0, . . . , V-1 } and class index y ∈ {0, 1}, the entry D(y,w) is the MAP estimate of θ<sub>yw</sub> = P(X<sub>w</sub> = 1|Y = y) with a Beta(2,1) prior distribution. 
<br>
<code>
θ<sub>yw</sub> =  #of X<sub>w1</sub> + (2 - 1) / (#of X<sub>w1</sub> + (2 - 1) + #of X<sub>w2</sub> + (1 - 1)
<pre>
def XGivenY(XTrain, ytrain):
    yeq1 = ytrain['0'] == 0
    yeq2 = ytrain['0'] == 1
    XTraineq1 = XTrain[yeq1].values
    XTraineq2 = XTrain[yeq2].values
    estProb = np.array([
        (sum(XTraineq1) + 1)/(1 + XTraineq1.shape[0]),
        (sum(XTraineq2) + 1)/(1 + XTraineq2.shape[0]),        
    ])
    return estProb
</pre>
</code>

### Classify(Estprob,p,Xtest)
The input X is an m × V matrix containing m feature vectors (stored as its rows). The output yHat is a m × 1 vector of predicted class labels, where yHat(i) is the predicted label for the i<sub>t</sub>h row of X. 
<code><pre>
def classify(EstProb,p,Xtest):
    '''
    Beta is Assumed to be (2,1)
    '''
    testSize = Xtest.shape[0]
    yhat = np.zeros(testSize,dtype=int)
    Xtest = Xtest.values
    for i in range(testSize):
        thetaechno = EstProb[0] * Xtest[i] + (1-EstProb[0]) * (1-Xtest[i])
        thetaonion = EstProb[1] * Xtest[i] + (1-EstProb[1]) * (1-Xtest[i])
        echnoScore = logProd.logProd(np.array([np.log(p),np.log(thetaechno).sum()]))
        onionScore = logProd.logProd(np.array([np.log(1-p),np.log(thetaonion).sum()]))    
        if onionScore > echnoScore:
            yhat[i] = 1
    return pd.DataFrame(yhat.reshape(-1,1))
    </pre>
    </code>
### Classification Error
Takes two vectors of equal length and returns the proportion of entries that they disagree on.
<code>
<pre>
def classificationError(yhat, ytrue):
    '''
    yhat : It Must Be Dataframe (n *1)
    ytrue: It must be Data frame (n *1)
    return: Return the error rate
    '''
    return (yhat != ytrue).sum()/len(yhat)</pre>
</code>


### Main
You can run main.py to see the working










































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































